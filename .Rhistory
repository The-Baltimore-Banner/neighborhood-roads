rm(full_accidents)
#these streets have neighborhood pairings that are less than 5.
low_neighborhood_street_pairing_counts_for_top_10 <- few_accidents %>%
group_by(clean_name) %>%
summarise(accidents = sum(accidents)) %>%
arrange(desc(accidents)) %>%
filter(clean_name %in% top_10_accidents_per_1k$clean_name)
temp <- top_10_accidents_per_1k %>% select(clean_name, accidents, accidents_per_1k, x2020_pop) %>% rename(counted_accidents = accidents)
low_neighborhood_street_pairing_counts_for_top_10 %>%
rename(possibly_missing_accidents = accidents) %>%
left_join(temp, by = c("clean_name" = "clean_name")) %>%
mutate(possible_accidents_per_1k = ((counted_accidents+possibly_missing_accidents)/(x2020_pop/1000))) %>%
mutate(difference_from_accidents_per_1k = possible_accidents_per_1k - accidents_per_1k) %>%
select(-x2020_pop) %>%
arrange(desc(possible_accidents_per_1k))
rm(temp)
# Number of accidents that may be missing from the calculation of the top 10 neighborhood/street pairings.
sum(low_neighborhood_street_pairing_counts_for_top_10$accidents)
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(janitor)
library(lubridate)
library(tidycensus)
library(readxl)
library(corrr)
library(sqldf)
library(scales)
library(psych)
options(digits=3)
options(scipen=999)
'%notin%' <- Negate('%in%')
#does a standard group_by and count() with percentage
grouper <- function(input_df, group_by_column, new_column_name = "n()"){
output_df <- input_df %>%
group_by(.data[[group_by_column]]) %>%
summarise(temp_count = n()) %>%
mutate(percent = temp_count/sum(temp_count)*100) %>%
arrange(desc(percent)) %>%
rename(!!new_column_name := temp_count)
return(output_df)
}
#group/counts every column in input dataframe
group_count <- function(input_df, group_column_name='n()', state_filter=NA, start_col = 1){
column_names <- colnames(input_df)
if(!is.na(state_filter)){
input_df <- input_df %>%
filter(state == state_filter)
}
for (column in column_names[start_col:length(column_names)]){
output <- grouper(input_df, column, group_column_name)
print(output)
}
}
#function for calculating age, updates to the day; stolen from the internet
calc_age <- function(birthDate, refDate = Sys.Date()) {
require(lubridate)
period <- as.period(interval(birthDate, refDate),
unit = "year")
period$year
}
#lowers case of every character column in a dataframe
lower_df <- function(input_df){
names <- colnames(input_df)
output_df <- input_df
names <- colnames(output_df)
for (name in names){
if (is.character(output_df[[name]])){
output_df[[name]] <- tolower(output_df[[name]])
#print('yes')
} else {
output_df[[name]] <- output_df[[name]]
#print('no')
}
}
return(output_df)
}
#imports every file in a folder
import_files_in_folder <- function(input_path, output_name=''){
files <- list.files(path=input_path)
for (file in files){you
file_path = paste0(input_path, file)
file_sans_csv = str_remove(file, '.csv')
file_sans_csv = gsub("-", "_", file_sans_csv)
imported_file <- read_csv(file_path)
assign(paste0(file_sans_csv, output_name), imported_file, envir = parent.frame())
}
}
import_bind_files_in_folder <- function(input_path, output_name=''){
files <- list.files(path=input_path)
master <- read_csv(paste0(input_path, files[1]))
for (file in files[2:length(files)]){
if (str_sub(file, -3) == 'csv'){
binder <- read_csv(paste0(input_path, file))
master <- master %>%
rbind(binder)
} else {
#pass
}
}
return(master)
}
grouper_sum <- function(input_df, group_by_column, sum_column, new_column_name = "n()"){
output_df <- input_df %>%
group_by(.data[[group_by_column]]) %>%
summarise(temp_count = sum(.data[[sum_column]])) %>%
mutate(percent = temp_count/sum(temp_count)*100) %>%
arrange(desc(percent)) %>%
rename(!!new_column_name := temp_count)
return(output_df)
}
#Import 2020 Census from BigLocalNews AP partnership
census <- read_csv("data/prop_change_by_neighborhood.csv") %>%
lower_df() %>%
select(neighborhood, x2020_pop)
#Import Baltimore DOT road classifications
road_classifications <- read_csv("data/baltimore-road-classifications-clean.csv") %>%
clean_names() %>%
lower_df() %>%
ungroup() %>%
mutate(clean_name = str_replace(fullname, paste0("", dirpre, " "), '')) %>%
mutate(clean_name = str_replace(clean_name, paste0(" ", dirsur, ""), '')) %>%
mutate(clean_name = str_replace(clean_name, "i ", "i")) %>%
mutate(clean_name = trimws(clean_name, which = c("both")))
#creating list of principal arteries for filtering
principal_arteries <- road_classifications %>%
filter(!is.na(fullname) & sha_class == "part") %>%
select(clean_name, sha_class) %>%
unique()
#import accidents output from QGIS
accidents<- read_csv("data/baltimore-city-accidents-by-neighborhood.csv") %>%
clean_names() %>%
lower_df() %>%
mutate(acc_date = ymd(acc_date)) %>%
rename(neighborhood = name)
#write output to clean in OpenRefine
#temp <- accidents%>%
#  select(report_no, mainroad_name)
#write_csv(temp, "output/mainroad_names_for_parsing.csv")
#import cleaned road names
refined_addresses <- read_csv("data/street-name-parsed-refined-with-types.csv") %>%
select(-mainroad_name)
#join refined names
accidents<- accidents%>%
left_join(refined_addresses) %>%
#Replaced all '0' with 'o' during address parsing to repair common error and make key collision refinement more effective, removing here
mutate(clean_name = case_when(
str_detect(combined, '([:digit:])(\\o)') == TRUE ~ str_replace_all(combined, 'o', '0'),
TRUE ~ combined
))
#importing all Baltimore accidents even if they do not have a geospatial neighborhood join
full_accidents <- read_csv("data/baltimore-city-accidents.csv")
years <- c(2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022)
for (input_year in years){
print(grouper(accidents%>% filter(year == input_year), "neighborhood"))
}
#The downtown neighborhood always has the most accidents.
#Looking for more accidents in each road in each Baltimore neighborhood
grouped_accidents <- accidents%>%
#excluding roads where a clean name does not exist
filter(!is.na(clean_name)) %>%
group_by(clean_name, neighborhood) %>%
summarise(accidents = n()) %>%
clean_names() %>%
left_join(census) %>%
mutate(accidents = as.double(accidents)) %>%
mutate(accidents_per_1k = (accidents/(x2020_pop/1000))) %>%
ungroup() %>%
mutate(part = case_when(
clean_name %in% principal_arteries$clean_name ~ TRUE,
TRUE ~ FALSE
))
few_accidents <- grouped_accidents %>%
filter(accidents < 5)
#removing accidents that list a street name but have locations that are due to bad location information
grouped_accidents <- grouped_accidents %>%
filter(accidents > 5)
quantile = quantile(grouped_accidents$x2020_pop, na.rm = TRUE)[2]
quantile
mutated_grouped_accidents <- grouped_accidents %>%
#filtering for top 75%
#this is the quantile from line 227
filter(x2020_pop >= quantile) %>%
mutate(rank_raw = order(order(accidents, decreasing = TRUE))) %>%
mutate(rank_per_1k = rank(desc(accidents_per_1k))) %>%
arrange(rank_per_1k)
top_10_accidents_per_1k <- mutated_grouped_accidents  %>%
slice(1:10)%>%
#arrange(neighborhood)
arrange(desc(accidents_per_1k))
#write_csv(top_10_accidents_per_1k, 'output/top-10-accidents-per-1k.csv')
top_10_accidents_per_1k
for (input_year in years){
grouped_accidents <- accidents%>%
filter(!is.na(clean_name),
year == input_year) %>%
group_by(clean_name, neighborhood) %>%
summarise(accidents = n()) %>%
clean_names() %>%
left_join(census) %>%
mutate(accidents = as.double(accidents)) %>%
mutate(accidents_per_1k = (accidents/(x2020_pop/1000))) %>%
ungroup() %>%
#test <- grouped_accidents %>%
mutate(part = case_when(
clean_name %in% principal_arteries$clean_name ~ TRUE,
TRUE ~ FALSE
))
mutated_grouped_accidents <- grouped_accidents %>%
#filtering for top 75%
#this is the quantile from line 227
filter(x2020_pop >= quantile) %>%
mutate(rank_raw = order(order(accidents, decreasing = TRUE))) %>%
mutate(rank_per_1k = rank(desc(accidents_per_1k))) %>%
arrange(rank_per_1k)
output <- mutated_grouped_accidents %>%
slice(1:10)%>%
#arrange(neighborhood)
arrange(desc(accidents_per_1k)) %>%
rename(!!as.character(input_year) := clean_name)
print(output)
}
#Orleans St in Dunbar-Broadway and CARE have been in the top ten most accidents per resident In some years, CARE or Dunbar-Broadway have had the most accidents per resident. Orleans St in the CARE neighborhood has had the most in any year in 2017 and 2020. Dunbar-Broadway had the most per resident of any street in any neighborhood with 40 per 1,000 residents in 2015 and 2016. It is the current leader in 2022.
#Rows without clean street names
nrow(accidents %>% filter(is.na(clean_name)))
#Calculating difference between raw Baltimore crashes and those that were included in the neighborhood geospatial join.
nrow(full_accidents)-nrow(accidents)
#Calculating percent of crashes that did not have both clean street name and a corresponding geospatial location.
(26219/nrow(full_accidents)*100)
rm(full_accidents)
#these streets have neighborhood pairings that are less than 5.
low_neighborhood_street_pairing_counts_for_top_10 <- few_accidents %>%
group_by(clean_name) %>%
summarise(accidents = sum(accidents)) %>%
arrange(desc(accidents)) %>%
filter(clean_name %in% top_10_accidents_per_1k$clean_name)
temp <- top_10_accidents_per_1k %>% select(clean_name, accidents, accidents_per_1k, x2020_pop) %>% rename(counted_accidents = accidents)
low_neighborhood_street_pairing_counts_for_top_10 %>%
rename(possibly_missing_accidents = accidents) %>%
left_join(temp, by = c("clean_name" = "clean_name")) %>%
mutate(possible_accidents_per_1k = ((counted_accidents+possibly_missing_accidents)/(x2020_pop/1000))) %>%
mutate(difference_from_accidents_per_1k = possible_accidents_per_1k - accidents_per_1k) %>%
select(-x2020_pop) %>%
arrange(desc(possible_accidents_per_1k))
rm(temp)
# Number of accidents that may be missing from the calculation of the top 10 neighborhood/street pairings.
sum(low_neighborhood_street_pairing_counts_for_top_10$accidents)
#Looking for more accidents in each road in each Baltimore neighborhood
grouped_accidents <- accidents%>%
#excluding roads where a clean name does not exist
filter(!is.na(clean_name)) %>%
group_by(clean_name, neighborhood) %>%
summarise(accidents = n()) %>%
clean_names() %>%
left_join(census) %>%
mutate(accidents = as.double(accidents)) %>%
mutate(accidents_per_1k = (accidents/(x2020_pop/1000))) %>%
ungroup() %>%
mutate(part = case_when(
clean_name %in% principal_arteries$clean_name ~ TRUE,
TRUE ~ FALSE
))
#removing accidents that list a street name but have locations that are due to bad location information
grouped_accidents <- grouped_accidents %>%
filter(accidents > 5)
quantile = quantile(grouped_accidents$x2020_pop, na.rm = TRUE)[2]
quantile
mutated_grouped_accidents <- grouped_accidents %>%
#filtering for top 75%
#this is the quantile from line 227
filter(x2020_pop >= quantile) %>%
mutate(rank_raw = order(order(accidents, decreasing = TRUE))) %>%
mutate(rank_per_1k = rank(desc(accidents_per_1k))) %>%
arrange(rank_per_1k)
top_10_accidents_per_1k <- mutated_grouped_accidents  %>%
slice(1:10)%>%
#arrange(neighborhood)
arrange(desc(accidents_per_1k))
#write_csv(top_10_accidents_per_1k, 'output/top-10-accidents-per-1k.csv')
top_10_accidents_per_1k
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(janitor)
library(lubridate)
library(tidycensus)
library(readxl)
library(corrr)
library(sqldf)
library(scales)
library(psych)
options(digits=3)
options(scipen=999)
'%notin%' <- Negate('%in%')
#does a standard group_by and count() with percentage
grouper <- function(input_df, group_by_column, new_column_name = "n()"){
output_df <- input_df %>%
group_by(.data[[group_by_column]]) %>%
summarise(temp_count = n()) %>%
mutate(percent = temp_count/sum(temp_count)*100) %>%
arrange(desc(percent)) %>%
rename(!!new_column_name := temp_count)
return(output_df)
}
#group/counts every column in input dataframe
group_count <- function(input_df, group_column_name='n()', state_filter=NA, start_col = 1){
column_names <- colnames(input_df)
if(!is.na(state_filter)){
input_df <- input_df %>%
filter(state == state_filter)
}
for (column in column_names[start_col:length(column_names)]){
output <- grouper(input_df, column, group_column_name)
print(output)
}
}
#function for calculating age, updates to the day; stolen from the internet
calc_age <- function(birthDate, refDate = Sys.Date()) {
require(lubridate)
period <- as.period(interval(birthDate, refDate),
unit = "year")
period$year
}
#lowers case of every character column in a dataframe
lower_df <- function(input_df){
names <- colnames(input_df)
output_df <- input_df
names <- colnames(output_df)
for (name in names){
if (is.character(output_df[[name]])){
output_df[[name]] <- tolower(output_df[[name]])
#print('yes')
} else {
output_df[[name]] <- output_df[[name]]
#print('no')
}
}
return(output_df)
}
#imports every file in a folder
import_files_in_folder <- function(input_path, output_name=''){
files <- list.files(path=input_path)
for (file in files){you
file_path = paste0(input_path, file)
file_sans_csv = str_remove(file, '.csv')
file_sans_csv = gsub("-", "_", file_sans_csv)
imported_file <- read_csv(file_path)
assign(paste0(file_sans_csv, output_name), imported_file, envir = parent.frame())
}
}
import_bind_files_in_folder <- function(input_path, output_name=''){
files <- list.files(path=input_path)
master <- read_csv(paste0(input_path, files[1]))
for (file in files[2:length(files)]){
if (str_sub(file, -3) == 'csv'){
binder <- read_csv(paste0(input_path, file))
master <- master %>%
rbind(binder)
} else {
#pass
}
}
return(master)
}
grouper_sum <- function(input_df, group_by_column, sum_column, new_column_name = "n()"){
output_df <- input_df %>%
group_by(.data[[group_by_column]]) %>%
summarise(temp_count = sum(.data[[sum_column]])) %>%
mutate(percent = temp_count/sum(temp_count)*100) %>%
arrange(desc(percent)) %>%
rename(!!new_column_name := temp_count)
return(output_df)
}
#Import 2020 Census from BigLocalNews AP partnership
census <- read_csv("data/prop_change_by_neighborhood.csv") %>%
lower_df() %>%
select(neighborhood, x2020_pop)
#Import Baltimore DOT road classifications
road_classifications <- read_csv("data/baltimore-road-classifications-clean.csv") %>%
clean_names() %>%
lower_df() %>%
ungroup() %>%
mutate(clean_name = str_replace(fullname, paste0("", dirpre, " "), '')) %>%
mutate(clean_name = str_replace(clean_name, paste0(" ", dirsur, ""), '')) %>%
mutate(clean_name = str_replace(clean_name, "i ", "i")) %>%
mutate(clean_name = trimws(clean_name, which = c("both")))
#creating list of principal arteries for filtering
principal_arteries <- road_classifications %>%
filter(!is.na(fullname) & sha_class == "part") %>%
select(clean_name, sha_class) %>%
unique()
#import accidents output from QGIS
accidents<- read_csv("data/baltimore-city-accidents-by-neighborhood.csv") %>%
clean_names() %>%
lower_df() %>%
mutate(acc_date = ymd(acc_date)) %>%
rename(neighborhood = name)
#write output to clean in OpenRefine
#temp <- accidents%>%
#  select(report_no, mainroad_name)
#write_csv(temp, "output/mainroad_names_for_parsing.csv")
#import cleaned road names
refined_addresses <- read_csv("data/street-name-parsed-refined-with-types.csv") %>%
select(-mainroad_name)
#join refined names
accidents<- accidents%>%
left_join(refined_addresses) %>%
#Replaced all '0' with 'o' during address parsing to repair common error and make key collision refinement more effective, removing here
mutate(clean_name = case_when(
str_detect(combined, '([:digit:])(\\o)') == TRUE ~ str_replace_all(combined, 'o', '0'),
TRUE ~ combined
))
#importing all Baltimore accidents even if they do not have a geospatial neighborhood join
full_accidents <- read_csv("data/baltimore-city-accidents.csv")
years <- c(2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022)
for (input_year in years){
print(grouper(accidents%>% filter(year == input_year), "neighborhood"))
}
#The downtown neighborhood always has the most accidents.
#Looking for more accidents in each road in each Baltimore neighborhood
grouped_accidents <- accidents%>%
#excluding roads where a clean name does not exist
filter(!is.na(clean_name)) %>%
group_by(clean_name, neighborhood) %>%
summarise(accidents = n()) %>%
clean_names() %>%
left_join(census) %>%
mutate(accidents = as.double(accidents)) %>%
mutate(accidents_per_1k = (accidents/(x2020_pop/1000))) %>%
ungroup() %>%
mutate(part = case_when(
clean_name %in% principal_arteries$clean_name ~ TRUE,
TRUE ~ FALSE
))
few_accidents <- grouped_accidents %>%
filter(accidents < 5)
#removing accidents that list a street name but have locations that are due to bad location information
grouped_accidents <- grouped_accidents %>%
filter(accidents > 5)
quantile = quantile(grouped_accidents$x2020_pop, na.rm = TRUE)[2]
quantile
mutated_grouped_accidents <- grouped_accidents %>%
#filtering for top 75%
#this is the quantile from line 227
filter(x2020_pop >= quantile) %>%
mutate(rank_raw = order(order(accidents, decreasing = TRUE))) %>%
mutate(rank_per_1k = rank(desc(accidents_per_1k))) %>%
arrange(rank_per_1k)
top_10_accidents_per_1k <- mutated_grouped_accidents  %>%
slice(1:10)%>%
#arrange(neighborhood)
arrange(desc(accidents_per_1k))
#write_csv(top_10_accidents_per_1k, 'output/top-10-accidents-per-1k.csv')
top_10_accidents_per_1k
for (input_year in years){
grouped_accidents <- accidents%>%
filter(!is.na(clean_name),
year == input_year) %>%
group_by(clean_name, neighborhood) %>%
summarise(accidents = n()) %>%
clean_names() %>%
left_join(census) %>%
mutate(accidents = as.double(accidents)) %>%
mutate(accidents_per_1k = (accidents/(x2020_pop/1000))) %>%
ungroup() %>%
#test <- grouped_accidents %>%
mutate(part = case_when(
clean_name %in% principal_arteries$clean_name ~ TRUE,
TRUE ~ FALSE
))
mutated_grouped_accidents <- grouped_accidents %>%
#filtering for top 75%
#this is the quantile from line 227
filter(x2020_pop >= quantile) %>%
mutate(rank_raw = order(order(accidents, decreasing = TRUE))) %>%
mutate(rank_per_1k = rank(desc(accidents_per_1k))) %>%
arrange(rank_per_1k)
output <- mutated_grouped_accidents %>%
slice(1:10)%>%
#arrange(neighborhood)
arrange(desc(accidents_per_1k)) %>%
rename(!!as.character(input_year) := clean_name)
print(output)
}
#Orleans St in Dunbar-Broadway and CARE have been in the top ten most accidents per resident In some years, CARE or Dunbar-Broadway have had the most accidents per resident. Orleans St in the CARE neighborhood has had the most in any year in 2017 and 2020. Dunbar-Broadway had the most per resident of any street in any neighborhood with 40 per 1,000 residents in 2015 and 2016. It is the current leader in 2022.
#Rows without clean street names
nrow(accidents %>% filter(is.na(clean_name)))
#Calculating difference between raw Baltimore crashes and those that were included in the neighborhood geospatial join.
nrow(full_accidents)-nrow(accidents)
#Calculating percent of crashes that did not have both clean street name and a corresponding geospatial location.
(26219/nrow(full_accidents)*100)
rm(full_accidents)
#these streets have neighborhood pairings that are less than 5.
low_neighborhood_street_pairing_counts_for_top_10 <- few_accidents %>%
group_by(clean_name) %>%
summarise(accidents = sum(accidents)) %>%
arrange(desc(accidents)) %>%
filter(clean_name %in% top_10_accidents_per_1k$clean_name)
temp <- top_10_accidents_per_1k %>% select(clean_name, accidents, accidents_per_1k, x2020_pop) %>% rename(counted_accidents = accidents)
low_neighborhood_street_pairing_counts_for_top_10 %>%
rename(possibly_missing_accidents = accidents) %>%
left_join(temp, by = c("clean_name" = "clean_name")) %>%
mutate(possible_accidents_per_1k = ((counted_accidents+possibly_missing_accidents)/(x2020_pop/1000))) %>%
mutate(difference_from_accidents_per_1k = possible_accidents_per_1k - accidents_per_1k) %>%
select(-x2020_pop) %>%
arrange(desc(possible_accidents_per_1k))
rm(temp)
# Number of accidents that may be missing from the calculation of the top 10 neighborhood/street pairings.
sum(low_neighborhood_street_pairing_counts_for_top_10$accidents)
